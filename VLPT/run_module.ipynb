{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.util\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 30086, 6, 141, 32, 47, 38, 524, 2051]\n",
      "'</s>'\n",
      "'Hi'\n",
      "','\n",
      "' how'\n",
      "' are'\n",
      "' you'\n",
      "' I'\n",
      "' am'\n",
      "' fine'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "tokens = tokenizer('Hi, how are you I am fine')['input_ids']\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "for i in tokens:\n",
    "    print(\"'\"+tokenizer.decode(i)+\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset\n",
      "18\n",
      "shakespeare-hamlet.txt\n",
      "tokenizing\n",
      "51667\n",
      "creatin batch 8 200\n",
      "LM\n",
      "tensor(4.6932, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "####### find best silence token - which token, when inserted randomly into text, preserves prediction performance best?\n",
    "\n",
    "from transformers import OPTForCausalLM\n",
    "import random\n",
    "import gc\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "\n",
    "print('creating dataset')\n",
    "import nltk\n",
    "files = nltk.corpus.gutenberg.fileids()\n",
    "from nltk.corpus import gutenberg\n",
    "dataset = ''\n",
    "\n",
    "#random.shuffle(files)\n",
    "#for file in files[0:10]:\n",
    "#    doc = gutenberg.raw(file)[0:100000]\n",
    "#    dataset+=doc\n",
    "print(len(files))\n",
    "dataset = gutenberg.raw(files[15]) # just use first jane austen book\n",
    "print(files[15])\n",
    "\n",
    "print('tokenizing')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "token_ids = tokenizer(dataset)['input_ids']\n",
    "del dataset\n",
    "print(len(token_ids))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SILENCE_TOKEN = 1437 #comma=6. newline character=50118. single space=1437   # run=422 : for stress test purposes\n",
    "batch_size=8\n",
    "num_tokens = 200\n",
    "print(\"creatin batch\", batch_size, num_tokens)\n",
    "LM_input = torch.zeros([batch_size,num_tokens], dtype=torch.long)\n",
    "random.seed(1)\n",
    "for b in range(batch_size):\n",
    "\n",
    "    # get random tokens from text\n",
    "    start=random.randint(0,len(token_ids)-num_tokens)\n",
    "    end=start+num_tokens\n",
    "    LM_input[b,:] = torch.tensor(token_ids[start:end], dtype=torch.long)\n",
    "\n",
    "    # insert silence tokens\n",
    "    for t in range(num_tokens-2): # leave last two tokens for prediction\n",
    "        if random.random() < 0.1: #0.059885932\n",
    "            LM_input[b,t:num_tokens].roll(1)\n",
    "            LM_input[b,t] = SILENCE_TOKEN\n",
    "            pass\n",
    "\n",
    "LM_input[:,0]=2\n",
    "\n",
    "print(\"LM\")\n",
    "labels = torch.full(LM_input.shape, -100, dtype=torch.long)\n",
    "labels[:,num_tokens-2] = LM_input[:,num_tokens-1]\n",
    "\n",
    "output = model(input_ids=LM_input, labels=LM_input)\n",
    "\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gated cross attention\n",
    "# make module\n",
    "\n",
    "from lib.masked_attention import MaskedAttention\n",
    "#REQUIRES:\n",
    "import functools\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import lib.xf as xf\n",
    "from lib.minecraft_util import store_args\n",
    "from lib.tree_util import tree_map\n",
    "\n",
    "mask_attention_module = MaskedAttention(\n",
    "        input_size=128,\n",
    "        memory_size=256,\n",
    "        heads=2,\n",
    "        timesteps=128,\n",
    "        mask = \"clipped_causal\",\n",
    "        init_scale=1,\n",
    "        norm=\"none\",\n",
    "        log_scope=\"sa\",\n",
    "        use_muP_factor=False) \n",
    "\n",
    "# put matrix through module\n",
    "\n",
    "import torch as th\n",
    "\n",
    "array = th.normal(0,1,(1000,128,16,16))\n",
    "print(array.mean())\n",
    "array2 = array[:,0,None,None]\n",
    "print(array2.shape)\n",
    "print(array2.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test token timings and how often silence occurs\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "result = 0\n",
    "for i in range(500):\n",
    "    prob_word = 0.1666666666\n",
    "    d=10\n",
    "    total = 0\n",
    "    num_shown = 0\n",
    "    skipped=0\n",
    "\n",
    "    random_seq = []\n",
    "    for i in range(2048):\n",
    "        #random_seq.append(random.gauss(0.5,0.335)<prob_word)   #  np.random.uniform(0,1,2048)\n",
    "        random_seq.append(random.random()<prob_word)   #  np.random.uniform(0,1,2048)\n",
    "        \n",
    "\n",
    "    for i in range(2048):\n",
    "\n",
    "        if i%d==0 and i>0:\n",
    "            token=np.max(random_seq[i-d:i])\n",
    "            if (token < np.sum(random_seq[i-d:i])):\n",
    "                skipped+=1\n",
    "            total+=token\n",
    "            num_shown += 1 #for every group checked only output 1\n",
    "\n",
    "    result+=total/num_shown\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5771c508f711f7da473bf0954744d30385b19c5daa239986e839c4dfdaff2669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
