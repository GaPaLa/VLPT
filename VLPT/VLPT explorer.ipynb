{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1669722448788,
     "user": {
      "displayName": "Idmi",
      "userId": "02220947904692487762"
     },
     "user_tz": 0
    },
    "id": "nbezCFQUbal_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CPU\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys, os\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "#from data_loader import DataLoader\n",
    "from lib.tree_util import tree_map\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    DEVICE = th.device('cuda')\n",
    "    print(\"USING CUDA\")\n",
    "else:\n",
    "    DEVICE = th.device('cpu')\n",
    "    print(\"USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([240])\n",
      "1 torch.Size([240])\n",
      "2 torch.Size([48, 5])\n",
      "3 torch.Size([48, 5])\n",
      "final 12\n",
      "{'input_ids': tensor([[ 6415,     2,   222,   617,   237,    28,  2452,     2,    68,    61,\n",
      "          4867,    38,    35, 27875,    80,    29,   138,   220,     3,   147,\n",
      "            29,  3024,     2,    68,  2271,    38,  1302,  2452,     2,     5,\n",
      "           138,   767,    29,  7116, 28661,     6, 10966,   304,     3]])}\n",
      "Well, if my name were Daniel, I would certainly not be joking about it so much. As it stands, I am not names Daniel, and so find it perfectly apt to mock you.\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"= = Distribution = =\n",
    "Species range across the Neotropics from Mexico in the north to Bolivia, Paraguay, and southern Brazil in the south. According to <unk> and coauthors, three\n",
    "species are found in Mexico, four in Central America, and 62 in South America. Three species are present in the Caribbean â€” two in Trinidad and Tobago, along\n",
    "the southern edge of the region, and one in Haiti.\"\"\"\n",
    "import numpy as np\n",
    "path = 'text.txt'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if True and idx > 0 and idx % 500000 == 0:\n",
    "            pass\n",
    "            #print('    line {}'.format(idx))\n",
    "        #print('LINE',line)\n",
    "\n",
    "\n",
    "data = th.LongTensor(np.arange(5*16*3))\n",
    "\n",
    "bsz = 5\n",
    "bptt = 4\n",
    "ext_len = 0\n",
    "\n",
    "# Work out how cleanly we can divide the dataset into bsz parts.\n",
    "print('0',data.shape)\n",
    "n_step = data.size(0) // bsz\n",
    "\n",
    "# Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "data = data.narrow(0, 0, n_step * bsz)\n",
    "print('1',data.shape)\n",
    "\n",
    "\n",
    "# Evenly divide the data across the bsz batches.\n",
    "data = data.view(bsz, -1).t().contiguous()\n",
    "print('2',data.shape)\n",
    "\n",
    "# Number of mini-batches\n",
    "n_batch = (n_step + bptt - 1) // bptt\n",
    "print('3',data.shape)\n",
    "\n",
    "print('final', n_batch)\n",
    "\n",
    "# LOAD TOKENIZER\n",
    "from transformers import TransfoXLTokenizer\n",
    "text = \"Well, if my name were Daniel, I would certainly not be joking about it so much. As it stands, I am not names Daniel, and so find it perfectly apt to mock you.\"\n",
    "\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")\n",
    "## TEST TOKENIZER\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "n_tokens = inputs['input_ids'].shape[1]\n",
    "print(inputs)\n",
    "inputs_list = inputs['input_ids'].reshape([n_tokens]).tolist()\n",
    "print(tokenizer.decode(inputs_list))\n",
    "print(tokenizer.decode([0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: USING CUSTOM TRANSFORMER_XL UTILITIES.py. DO NOT USE THE LMHEAD.FORWAD() METHOD IWTH LABELS, IT WILL GIVE INCORRECT LOSS AND PREDICTIONS\n",
      "1024\n",
      "18\n",
      "16\n",
      "1024\n",
      "None\n",
      "267735\n",
      "-1\n",
      "4\n",
      "AdaptiveEmbedding(\n",
      "  (emb_layers): ModuleList(\n",
      "    (0): Embedding(20000, 1024)\n",
      "    (1): Embedding(20000, 256)\n",
      "    (2): Embedding(160000, 64)\n",
      "    (3): Embedding(67735, 16)\n",
      "  )\n",
      "  (emb_projs): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
      "  )\n",
      ")\n",
      "AdaptiveEmbedding(\n",
      "  (emb_layers): ModuleList(\n",
      "    (0): Embedding(20000, 1024)\n",
      "    (1): Embedding(20000, 256)\n",
      "    (2): Embedding(160000, 64)\n",
      "    (3): Embedding(67735, 16)\n",
      "  )\n",
      "  (emb_projs): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
      "  )\n",
      ")\n",
      "False\n",
      "False\n",
      "285205322\n"
     ]
    }
   ],
   "source": [
    "#LOAD LM\n",
    "from transformers import TransfoXLLMHeadModel\n",
    "model = TransfoXLLMHeadModel.from_pretrained(\"transfo-xl-wt103\")\n",
    "\n",
    "\n",
    "print(model.transformer.d_embed)\n",
    "print(model.transformer.n_layer)\n",
    "print(model.transformer.n_head)\n",
    "print(model.transformer.d_model)\n",
    "print(model.transformer.config.bos_token_id)\n",
    "print(model.transformer.config.vocab_size)\n",
    "print(model.sample_softmax)\n",
    "print(model.transformer.config.div_val)\n",
    "print(model.transformer.word_emb)\n",
    "print(model.transformer.word_emb)\n",
    "\n",
    "print(model.config.torchscript)\n",
    "print(model.trainer_compatible)\n",
    "\n",
    "\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#outputs = model(inputs_embeds=torch.normal(0,1,[1,5,1024]))\n",
    "input_ids = th.tensor([[6415,     2,   222,   617,   237,    28,  2452,     2,    68,    61,\n",
    "          4867,    38,    35, 27875,    80,    29,   138,   220,     3,   147,\n",
    "            29,  3024,     2,    68,  2271,    38,  1302,  2452,     2,     5,\n",
    "           138,   767,    29,  7116, 28661,     6, 10966,   304,     3]],\n",
    "            dtype=th.long)\n",
    "\n",
    "outputs = model(input_ids=input_ids, labels=input_ids.roll(1, dims=1))\n",
    "#outputs2 = model(input_ids=input_ids, labels=input_ids)\n",
    "\n",
    "#\n",
    "\n",
    "#print(tokenizer.decode(outputs.prediction_scores[0,5,1000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -7.3768,  -4.0480,  -3.6218,  ..., -18.7565, -18.9308, -18.8882],\n",
      "        [ -2.2302,  -3.4663,  -3.5769,  ..., -16.8529, -16.6243, -16.0952],\n",
      "        [-11.5055,  -3.2825,  -6.6499,  ..., -18.9754, -20.5230, -18.4249],\n",
      "        ...,\n",
      "        [ -8.6760,  -1.9662,  -4.1156,  ..., -20.2708, -20.0940, -18.7047],\n",
      "        [ -8.4901,  -6.4914,  -2.0714,  ..., -22.3452, -21.8349, -20.8560],\n",
      "        [ -2.3459,  -3.8325,  -3.5754,  ..., -16.6095, -16.4205, -15.8427]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor(8.7466, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 39])\n",
      "torch.Size([39])\n",
      "Well, if my name were Daniel, I would certainly not be joking about it so much. As it stands, I am not names Daniel, and so find it perfectly apt to mock you. \n",
      "\n",
      "well <eos> not own is to Defoe I would have be be Daniel. it. I that <eos> a turned, it would not joking., I I I myself amusing appropriate to be the. <eos>\n"
     ]
    }
   ],
   "source": [
    "#print(outputs)\n",
    "print(outputs.logits)\n",
    "print(outputs.loss)\n",
    "\n",
    "\n",
    "maxes = th.argmax(outputs.logits, axis=1)\n",
    "print(input_ids.shape)\n",
    "print(maxes.shape)\n",
    "print(tokenizer.decode(input_ids[0]),\"\\n\")\n",
    "print(tokenizer.decode(maxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1669722218180,
     "user": {
      "displayName": "Idmi",
      "userId": "02220947904692487762"
     },
     "user_tz": 0
    },
    "id": "Wfv11S7AjPbo",
    "outputId": "dea05d6d-d32f-4dbd-fbf1-398affc96078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: USING CUSTOM TRANSFORMER_XL UTILITIES.py. DO NOT USE THE LMHEAD.FORWAD() METHOD IWTH LABELS, IT WILL GIVE INCORRECT LOSS AND PREDICTIONS\n",
      "2048\n",
      "{'active_reward_monitors': {'craft_stats': {'args': {'collapse_var': True, 'items': ['planks', 'stick', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe', 'furnace', 'iron_ingot', 'iron_pickaxe', 'diamond_pickaxe', 'torch']}, 'weight': 0}, 'mine_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal_ore', 'stone', 'iron_ore', 'diamond_ore', 'obsidian']}, 'weight': 0}, 'order_invariant_curriculum': {'args': {'curriculum': {'coal': [5, 0.4], 'cobblestone': [11, 0.09090909090909091], 'crafting_table': [1, 1], 'diamond': [10000, 2.6666666666666665], 'diamond_pickaxe': [10000, 8], 'furnace': [1, 1], 'iron_ingot': [3, 1.3333333333333333], 'iron_ore': [3, 1.3333333333333333], 'iron_pickaxe': [1, 4], 'log': [8, 0.125], 'obsidian': [10000, 16], 'planks': [20, 0.05], 'stick': [16, 0.0625], 'stone_pickaxe': [1, 1], 'torch': [16, 0.125], 'wooden_pickaxe': [1, 1]}}, 'weight': 1}, 'pickup_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal', 'cobblestone', 'iron_ore', 'diamond']}, 'weight': 0}, 'variety': {'args': {'collapse_var': True, 'included_items': ['beef', 'chicken', 'leather', 'mutton', 'porkchop', 'bucket', 'milk_bucket', 'water_bucket', 'coal', 'crafting_table', 'furnace', 'diamond', 'gold_ingot', 'gold_ore', 'flint', 'iron_ingot', 'iron_ore', 'shears', 'string', 'cobblestone', 'log', 'planks', 'stick', 'wool', 'obsidian', 'paper', 'redstone', 'wheat', 'cooked_beef', 'cooked_chicken', 'cooked_mutton', 'cooked_porkchop', 'egg', 'feather', 'leather_boots', 'leather_chestplate', 'leather_helmet', 'leather_leggings', 'brick', 'brick_stairs', 'clay', 'clay_ball', 'flower_pot', 'terracotta', 'torch', 'diamond_axe', 'diamond_block', 'diamond_boots', 'diamond_chestplate', 'diamond_helmet', 'diamond_hoe', 'diamond_leggings', 'diamond_pickaxe', 'diamond_shovel', 'diamond_sword', 'dirt', 'golden_apple', 'golden_axe', 'golden_boots', 'golden_chestplate', 'golden_helmet', 'golden_hoe', 'golden_leggings', 'golden_pickaxe', 'golden_shovel', 'golden_sword', 'arrow', 'gravel', 'iron_axe', 'iron_boots', 'iron_chestplate', 'iron_helmet', 'iron_hoe', 'iron_leggings', 'iron_pickaxe', 'iron_shovel', 'iron_sword', 'shield', 'fermented_spider_eye', 'leaves', 'apple', 'bread', 'activator_rail', 'clock', 'compass', 'detector_rail', 'dropper', 'book', 'bookshelf', 'cake', 'filled_map', 'sugar_cane', 'sugar', 'bow', 'dispenser', 'fishing_rod', 'spider_eye', 'stone_axe', 'stone_hoe', 'stone_pickaxe', 'stone_shovel', 'stone_sword', 'boat', 'wooden_axe', 'wooden_hoe', 'wooden_pickaxe', 'wooden_shovel', 'wooden_sword', 'banner', 'bed', 'carpet', 'map', 'redstone_torch', 'wheat_seeds', 'flint_and_steel']}, 'weight': 0}}, 'attention_heads': 16, 'attention_mask_style': 'clipped_causal', 'attention_memory_size': 256, 'diff_mlp_embedding': False, 'hidsize': 2048, 'img_shape': [128, 128, 3], 'impala_chans': [16, 32, 32], 'impala_kwargs': {'post_pool_groups': 1}, 'impala_width': 8, 'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1}, 'n_recurrence_layers': 4, 'only_img_input': True, 'pointwise_ratio': 4, 'pointwise_use_activation': False, 'recurrence_is_residual': True, 'recurrence_type': 'transformer', 'timesteps': 128, 'use_pointwise_layer': True, 'use_pre_lstm_ln': False}\n"
     ]
    }
   ],
   "source": [
    "def load_model_parameters(path_to_model_file):\n",
    "    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n",
    "    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "\n",
    "    return policy_kwargs, pi_head_kwargs\n",
    "\n",
    "from agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "\n",
    "agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(\"2x.model\")\n",
    "\n",
    "env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
    "agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n",
    "env.close()\n",
    "policy = agent.policy\n",
    "trainable_parameters = policy.parameters()\n",
    "\n",
    "print(policy.net.hidsize)\n",
    "print(agent_policy_kwargs)\n",
    "\n",
    "\n",
    "#frames = agent._video_obs_to_agent(frames)\n",
    "#print(agent.predict_actions(np.repeat(frames,1,axis=0)))\n",
    "#print(agent.hidden_state[0])\n",
    "\n",
    "from agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words, frmes=4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idmi/miniconda3/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT1 in shape torch.Size([3, 4, 2048]) torch.Size([3, 4])\n",
      "VPT1 out shape torch.Size([3, 4, 2048])\n",
      "xattn1 out shape torch.Size([3, 2, 1024])\n",
      "LM words shape torch.Size([3, 2, 267735])\n",
      "fused output= torch.Size([3, 4, 2048])\n",
      "single out= False\n",
      "x= torch.Size([3, 4, 2048])\n",
      "WORDS SHAPE,  torch.Size([3, 2, 267735])\n",
      "{'camera': tensor([[[[-4.6900, -4.4223, -4.7174,  ..., -5.1454, -4.7894, -5.0858]],\n",
      "\n",
      "         [[-4.9052, -4.7045, -4.8177,  ..., -5.0853, -4.9800, -4.7797]],\n",
      "\n",
      "         [[-5.0652, -4.7052, -4.9896,  ..., -5.0257, -5.1446, -5.2268]],\n",
      "\n",
      "         [[-4.7226, -4.5823, -4.9358,  ..., -4.7649, -5.5776, -5.7678]]],\n",
      "\n",
      "\n",
      "        [[[-4.5158, -4.4628, -4.8123,  ..., -4.2481, -4.8369, -5.3228]],\n",
      "\n",
      "         [[-4.6616, -4.4600, -4.5524,  ..., -4.9918, -4.8437, -4.7191]],\n",
      "\n",
      "         [[-4.8252, -4.6400, -4.7190,  ..., -4.9857, -4.8744, -4.7552]],\n",
      "\n",
      "         [[-4.4890, -5.0943, -4.7446,  ..., -4.9267, -4.4971, -5.2114]]],\n",
      "\n",
      "\n",
      "        [[[-4.7008, -4.7886, -4.7055,  ..., -5.1179, -5.2914, -4.7973]],\n",
      "\n",
      "         [[-4.4156, -4.7018, -4.4920,  ..., -5.2544, -4.8255, -5.3006]],\n",
      "\n",
      "         [[-4.5587, -4.8586, -4.5392,  ..., -5.1481, -5.4456, -4.8012]],\n",
      "\n",
      "         [[-4.7720, -4.7035, -4.3184,  ..., -5.1789, -5.3282, -5.0173]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>), 'buttons': tensor([[[[-8.5948, -9.5190, -9.1777,  ..., -9.1161, -9.2038, -8.8982]],\n",
      "\n",
      "         [[-9.0071, -9.5109, -9.1910,  ..., -8.9408, -9.1415, -8.9875]],\n",
      "\n",
      "         [[-8.9044, -8.8361, -9.4696,  ..., -9.1449, -9.0490, -9.0566]],\n",
      "\n",
      "         [[-9.1216, -9.7445, -9.6306,  ..., -8.9957, -8.9116, -9.1872]]],\n",
      "\n",
      "\n",
      "        [[[-9.1253, -8.9202, -9.4932,  ..., -9.6699, -8.6737, -8.9640]],\n",
      "\n",
      "         [[-9.2248, -9.0381, -8.8268,  ..., -8.9097, -8.4904, -8.7182]],\n",
      "\n",
      "         [[-9.2572, -9.5345, -9.2923,  ..., -8.8400, -8.9779, -9.3440]],\n",
      "\n",
      "         [[-8.9476, -9.1886, -8.5821,  ..., -9.5412, -9.2600, -9.0286]]],\n",
      "\n",
      "\n",
      "        [[[-9.3705, -9.2634, -9.2393,  ..., -9.0730, -8.8048, -9.2397]],\n",
      "\n",
      "         [[-9.1968, -9.0376, -9.1756,  ..., -8.9884, -9.4383, -8.8676]],\n",
      "\n",
      "         [[-9.1071, -9.1620, -9.2011,  ..., -8.9903, -9.0423, -9.4806]],\n",
      "\n",
      "         [[-9.0195, -9.2035, -8.9116,  ..., -9.1881, -9.5517, -9.1997]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)}\n"
     ]
    }
   ],
   "source": [
    "# TEST POLICY RECURRENT/TRANSFORMER BEHAVIOUR\n",
    "seq_len = 4\n",
    "bsz = 3\n",
    "frameA = th.normal(0,1,[bsz,seq_len,128,128,3])\n",
    "frameB = th.normal(0,1,[bsz,seq_len,128,128,3])\n",
    "tokenA = th.full([bsz,seq_len],4127, dtype=th.long)\n",
    "tokenB = th.full([bsz,seq_len],3467, dtype=th.long)\n",
    "\n",
    "# get output for input P(A)\n",
    "frames = {'img':frameA,\n",
    "            'ms': th.zeros([bsz,seq_len])}\n",
    "\n",
    "words = {'token_ids':tokenA,\n",
    "            'ms': th.zeros([bsz,seq_len])}\n",
    "pi_latent, vf_latent, LM_words, VPT_state_out, LM_state_out, LM_loss = agent.policy.get_output_for_observations(ob_frames=frames, ob_words=words)\n",
    "print(pi_latent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   16, 12963],\n",
       "        [   16, 12963],\n",
       "        [   16, 12963]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_words.shape\n",
    "th.argmax(LM_words, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. x shape: torch.Size([2, 1, 128, 128, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idmi/miniconda3/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. x shape: torch.Size([2, 1, 4096])\n",
      "shape of IDM predict output: {'buttons': tensor([[[0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]]]), 'camera': tensor([[[3, 2]],\n",
      "\n",
      "        [[5, 4]]])}\n",
      "hidden state: [(None, (tensor([], size=(2, 0, 4096)), tensor([], size=(2, 0, 4096)))), (None, (tensor([], size=(2, 0, 4096)), tensor([], size=(2, 0, 4096))))]\n"
     ]
    }
   ],
   "source": [
    "# TEST IDM BEHAVIOUR\n",
    "\n",
    "from inverse_dynamics_model import IDMAgent\n",
    "\n",
    "agent_parameters = pickle.load(open('4x_idm.model', \"rb\"))\n",
    "net_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "agent = IDMAgent(device=DEVICE, idm_net_kwargs=net_kwargs, pi_head_kwargs=pi_head_kwargs)\n",
    "\n",
    "first = th.from_numpy(np.array((False,))).to(DEVICE)\n",
    "\n",
    "#frames = [np.random.normal(0,0.02,(128,128,3)),  np.random.normal(0,0.02,(128,128,3)),  np.random.normal(0,0.02,(128,128,3))]\n",
    "\n",
    "frames = np.random.normal(0,0.02,(2,1,128,128,3))\n",
    "ac = agent.predict_actions(frames, raw_frames=True) # test for frames=1 and n_frames=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(ac[\u001b[39m'\u001b[39m\u001b[39mcamera\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ac' is not defined"
     ]
    }
   ],
   "source": [
    "print(ac['camera'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TEST X-ATTN\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ob_frames \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mones((\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m3\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m0.01\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m ob_frames \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39m_video_obs_to_agent( ob_frames )\n\u001b[1;32m      5\u001b[0m ob_frames[\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m,:,:,:,:] \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones([\u001b[39m10\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m3\u001b[39m])\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.05\u001b[39m\n\u001b[1;32m      6\u001b[0m ob_words \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39m_words_to_agent(\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm going to count to 10: 1, 2, 3, 4, 5, 6, 7, 8, \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# TEST X-ATTN\n",
    "\n",
    "ob_frames = [np.ones((128,128,3))*0.01 for i in range(10)]\n",
    "ob_frames = agent._video_obs_to_agent( ob_frames )\n",
    "ob_frames['img'][0,:,:,:,:] = th.ones([10,128,128,3])*-0.05\n",
    "ob_words = agent._words_to_agent(\"I'm going to count to 10: 1, 2, 3, 4, 5, 6, 7, 8, \")\n",
    "\n",
    "policy.net.Xattn_VPT_LM.alpha_xattn=th.nn.Parameter(th.tensor(0.))\n",
    "policy.net.Xattn_VPT_LM.alpha_xattn=th.nn.Parameter(th.tensor(0.))\n",
    "\n",
    "action_prob, action, pd_word = policy.get_output_for_observations(ob_words=ob_words, ob_frames=ob_frames) # test for frames=1 and n_frames=2\n",
    "print(pd_word.mean(), pd_word.std())\n",
    "\n",
    "\n",
    "token_index = th.argmax(pd_word[0,-1,:]).item()\n",
    "\n",
    "print(agent._agent_words_to_string(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVhoKnqVM5Fd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 8\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 12\n",
    "LOSS_REPORT_RATE = 100\n",
    "LEARNING_RATE = 0.000181\n",
    "WEIGHT_DECAY = 0.039428\n",
    "MAX_GRAD_NORM = 5.0\n",
    "# Basic behavioural cloning\n",
    "# Note: this uses gradient accumulation in batches of ones\n",
    "#       to perform training.\n",
    "#       This will fit inside even smaller GPUs (tested on 8GB one),\n",
    "#       but is slow.\n",
    "# NOTE: This is _not_ the original code used for VPT!\n",
    "#       This is merely to illustrate how to fine-tune the models and includes\n",
    "#       the processing steps used.                                               @FIX THIS to run on 4x3090 = 96GB:\n",
    "\n",
    "# This will likely be much worse than what original VPT did:\n",
    "# we are not training on full sequences, but only one step at a time to save VRAM.\n",
    "def behavioural_cloning_train(data_dir, in_model, in_weights, out_weights):\n",
    "    agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n",
    "\n",
    "    # To create model with the right environment.\n",
    "    # All basalt environments have the same settings, so any of them works here\n",
    "    env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
    "    agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n",
    "    agent.load_weights(in_weights)\n",
    "    env.close()\n",
    "\n",
    "    policy = agent.policy\n",
    "    trainable_parameters = policy.parameters()\n",
    "\n",
    "    # Parameters taken from the OpenAI VPT paper\n",
    "    optimizer = th.optim.Adam(\n",
    "        trainable_parameters,\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset_dir=data_dir,\n",
    "        n_workers=N_WORKERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Keep track of the hidden state per episode/trajectory.\n",
    "    # DataLoader provides unique id for each episode, which will\n",
    "    # be different even for the same trajectory when it is loaded\n",
    "    # up again\n",
    "    episode_hidden_states = {}\n",
    "    dummy_first = th.from_numpy(np.array((False,))).to(DEVICE)\n",
    "\n",
    "    loss_sum = 0\n",
    "    for batch_i, (batch_images, batch_actions, batch_episode_id) in enumerate(data_loader):\n",
    "        batch_loss = 0\n",
    "        for image, action, episode_id in zip(batch_images, batch_actions, batch_episode_id):\n",
    "            \n",
    "            agent_action = agent._env_action_to_agent(action, to_torch=True, check_if_null=True)\n",
    "            \n",
    "            if agent_action is None:\n",
    "                # Action was null\n",
    "                continue\n",
    "\n",
    "            agent_obs = agent._video_obs_to_agent({\"pov\": image})\n",
    "\n",
    "            if episode_id not in episode_hidden_states:\n",
    "                # TODO need to clean up this hidden state after worker is done with the work item.\n",
    "                #      Leaks memory, but not tooooo much at these scales (will be a problem later).\n",
    "                episode_hidden_states[episode_id] = policy.initial_state(1)\n",
    "            agent_state = episode_hidden_states[episode_id]\n",
    "            \n",
    "            pi_distribution, v_prediction, new_agent_state, word_pd, word_v = policy.get_output_for_observations(\n",
    "                ob_frames = agent_obs,\n",
    "                ob_words = ob_words,\n",
    "                VPT_state_in = agent_state,\n",
    "                dummy_first = None\n",
    "            )\n",
    "\n",
    "            if (LM_full):\n",
    "                loss = \n",
    "\n",
    "\n",
    "            # ACTION LOSS\n",
    "            log_prob  = policy.get_logprob_of_action(pi_distribution, agent_action)\n",
    "\n",
    "\n",
    "            # LANGUAGE LOSS - from modeling_opt.py\n",
    "            logits = word_pd.contiguous()\n",
    "            labels = th.full([word_pd.shape[0],word_pd.shape[1],1], -100, dtype=th.LongTensor) # dont compute loss for masked tokens (hence -100 token ids for masked tokens, as per OPT)\n",
    "            for b in range(len(ob_words['input_ids'])):\n",
    "                n_tokens = len(ob_words['input_ids'][b])\n",
    "                labels[:n_tokens] = ob_words['input_ids']\n",
    "            loss = None\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()      #@ make sure to not train \n",
    "            # Flatten the tokens\n",
    "            loss_fct = th.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, policy.net.LM.model.decoder.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Make sure we do not try to backprop through sequence\n",
    "            # (fails with current accumulation)\n",
    "            new_agent_state = tree_map(lambda x: x.detach(), new_agent_state)\n",
    "            episode_hidden_states[episode_id] = new_agent_state\n",
    "\n",
    "            # Finally, update the agent to increase the probability of the\n",
    "            # taken action.\n",
    "            # Remember to take mean over batch losses\n",
    "            loss = -log_prob / BATCH_SIZE\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "        th.nn.utils.clip_grad_norm_(trainable_parameters, MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += batch_loss\n",
    "        if batch_i % LOSS_REPORT_RATE == 0:\n",
    "            time_since_start = time.time() - start_time\n",
    "            print(f\"Time: {time_since_start:.2f}, Batches: {batch_i}, Avrg loss: {loss_sum / LOSS_REPORT_RATE:.4f}\")\n",
    "            loss_sum = 0\n",
    "\n",
    "    state_dict = policy.state_dict()\n",
    "    th.save(state_dict, out_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "attention_heads': 16,\n",
    "  'attention_mask_style': 'clipped_causal',\n",
    "  'attention_memory_size': 256,\n",
    "  'diff_mlp_embedding': False,\n",
    "  'hidsize': 2048,\n",
    "  'img_shape': [128, 128, 3],\n",
    "  'impala_chans': [16, 32, 32],\n",
    "  'impala_kwargs': {'post_pool_groups': 1},\n",
    "  'impala_width': 8,\n",
    "  'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1},\n",
    "  'n_recurrence_layers': 4,\n",
    "  'only_img_input': True,\n",
    "  'pointwise_ratio': 4,\n",
    "  'pointwise_use_activation': False,\n",
    "  'recurrence_is_residual': True,\n",
    "  'recurrence_type': 'transformer',\n",
    "  'timesteps': 128,\n",
    "  'use_pointwise_layer': True,\n",
    "  'use_pre_lstm_ln': False},\n",
    "\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = th.nn.MultiheadAttention(\n",
    "            embed_dim=512,  # in pytorch, its split across heads\n",
    "            num_heads=16,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            add_zero_attn=False,\n",
    "            kdim=128*16,vdim=128*16,\n",
    "            batch_first=True,\n",
    "            device=DEVICE,\n",
    "            dtype=th.float32)\n",
    "\n",
    "tokensL = th.normal(0,0.02,[2,4,512])\n",
    "tokensV = th.normal(0,0.02,[2,5,2048])\n",
    "\n",
    "attn_mask = th.zeros([4,5])\n",
    "attn_mask[-1,:] = 1\n",
    "print(attn_mask)\n",
    "\n",
    "out = attention(query=tokensL, key=tokensV, value=tokensV, attn_mask=attn_mask)\n",
    "print(out)\n",
    "th.isnan(out[0]).any()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMW0NSERlVQQUPphW+YKITN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('minerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5771c508f711f7da473bf0954744d30385b19c5daa239986e839c4dfdaff2669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
