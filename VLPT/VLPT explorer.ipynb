{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1669722448788,
     "user": {
      "displayName": "Idmi",
      "userId": "02220947904692487762"
     },
     "user_tz": 0
    },
    "id": "nbezCFQUbal_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CPU\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys, os\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "#from data_loader import DataLoader\n",
    "from lib.tree_util import tree_map\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    DEVICE = th.device('cuda')\n",
    "    print(\"USING CUDA\")\n",
    "else:\n",
    "    DEVICE = th.device('cpu')\n",
    "    print(\"USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g2kZv2mkM7IL"
   },
   "outputs": [],
   "source": [
    "def load_model_parameters(path_to_model_file):\n",
    "    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n",
    "    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "\n",
    "    return policy_kwargs, pi_head_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1669722218180,
     "user": {
      "displayName": "Idmi",
      "userId": "02220947904692487762"
     },
     "user_tz": 0
    },
    "id": "Wfv11S7AjPbo",
    "outputId": "dea05d6d-d32f-4dbd-fbf1-398affc96078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "{'active_reward_monitors': {'craft_stats': {'args': {'collapse_var': True, 'items': ['planks', 'stick', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe', 'furnace', 'iron_ingot', 'iron_pickaxe', 'diamond_pickaxe', 'torch']}, 'weight': 0}, 'mine_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal_ore', 'stone', 'iron_ore', 'diamond_ore', 'obsidian']}, 'weight': 0}, 'order_invariant_curriculum': {'args': {'curriculum': {'coal': [5, 0.4], 'cobblestone': [11, 0.09090909090909091], 'crafting_table': [1, 1], 'diamond': [10000, 2.6666666666666665], 'diamond_pickaxe': [10000, 8], 'furnace': [1, 1], 'iron_ingot': [3, 1.3333333333333333], 'iron_ore': [3, 1.3333333333333333], 'iron_pickaxe': [1, 4], 'log': [8, 0.125], 'obsidian': [10000, 16], 'planks': [20, 0.05], 'stick': [16, 0.0625], 'stone_pickaxe': [1, 1], 'torch': [16, 0.125], 'wooden_pickaxe': [1, 1]}}, 'weight': 1}, 'pickup_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal', 'cobblestone', 'iron_ore', 'diamond']}, 'weight': 0}, 'variety': {'args': {'collapse_var': True, 'included_items': ['beef', 'chicken', 'leather', 'mutton', 'porkchop', 'bucket', 'milk_bucket', 'water_bucket', 'coal', 'crafting_table', 'furnace', 'diamond', 'gold_ingot', 'gold_ore', 'flint', 'iron_ingot', 'iron_ore', 'shears', 'string', 'cobblestone', 'log', 'planks', 'stick', 'wool', 'obsidian', 'paper', 'redstone', 'wheat', 'cooked_beef', 'cooked_chicken', 'cooked_mutton', 'cooked_porkchop', 'egg', 'feather', 'leather_boots', 'leather_chestplate', 'leather_helmet', 'leather_leggings', 'brick', 'brick_stairs', 'clay', 'clay_ball', 'flower_pot', 'terracotta', 'torch', 'diamond_axe', 'diamond_block', 'diamond_boots', 'diamond_chestplate', 'diamond_helmet', 'diamond_hoe', 'diamond_leggings', 'diamond_pickaxe', 'diamond_shovel', 'diamond_sword', 'dirt', 'golden_apple', 'golden_axe', 'golden_boots', 'golden_chestplate', 'golden_helmet', 'golden_hoe', 'golden_leggings', 'golden_pickaxe', 'golden_shovel', 'golden_sword', 'arrow', 'gravel', 'iron_axe', 'iron_boots', 'iron_chestplate', 'iron_helmet', 'iron_hoe', 'iron_leggings', 'iron_pickaxe', 'iron_shovel', 'iron_sword', 'shield', 'fermented_spider_eye', 'leaves', 'apple', 'bread', 'activator_rail', 'clock', 'compass', 'detector_rail', 'dropper', 'book', 'bookshelf', 'cake', 'filled_map', 'sugar_cane', 'sugar', 'bow', 'dispenser', 'fishing_rod', 'spider_eye', 'stone_axe', 'stone_hoe', 'stone_pickaxe', 'stone_shovel', 'stone_sword', 'boat', 'wooden_axe', 'wooden_hoe', 'wooden_pickaxe', 'wooden_shovel', 'wooden_sword', 'banner', 'bed', 'carpet', 'map', 'redstone_torch', 'wheat_seeds', 'flint_and_steel']}, 'weight': 0}}, 'attention_heads': 16, 'attention_mask_style': 'clipped_causal', 'attention_memory_size': 256, 'diff_mlp_embedding': False, 'hidsize': 2048, 'img_shape': [128, 128, 3], 'impala_chans': [16, 32, 32], 'impala_kwargs': {'post_pool_groups': 1}, 'impala_width': 8, 'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1}, 'n_recurrence_layers': 4, 'only_img_input': True, 'pointwise_ratio': 4, 'pointwise_use_activation': False, 'recurrence_is_residual': True, 'recurrence_type': 'transformer', 'timesteps': 128, 'use_pointwise_layer': True, 'use_pre_lstm_ln': False}\n"
     ]
    }
   ],
   "source": [
    "from agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "\n",
    "agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(\"2x.model\")\n",
    "\n",
    "env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
    "agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n",
    "env.close()\n",
    "policy = agent.policy\n",
    "trainable_parameters = policy.parameters()\n",
    "\n",
    "print(policy.net.hidsize)\n",
    "print(agent_policy_kwargs)\n",
    "\n",
    "\n",
    "#frames = agent._video_obs_to_agent(frames)\n",
    "#print(agent.predict_actions(np.repeat(frames,1,axis=0)))\n",
    "#print(agent.hidden_state[0])\n",
    "\n",
    "from agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words, frmes=1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idmi/miniconda3/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT1 out shape torch.Size([1, 1, 2048])\n",
      "xattn1 out shape torch.Size([1, 1, 1024])\n",
      "LM out shape torch.Size([1, 1, 1024])\n",
      "LM repeated out shape torch.Size([1, 1, 1024])\n",
      "fused output= torch.Size([1, 1, 2048])\n",
      "single out= False\n",
      "x= torch.Size([1, 1, 2048])\n",
      "{'camera': tensor([[[[-4.8138, -4.9176, -5.1456, -4.7176, -4.8106, -5.0663, -5.2474,\n",
      "           -5.2863, -5.2326, -5.1281, -4.8281, -4.8934, -4.8535, -4.7908,\n",
      "           -4.8386, -5.0008, -4.5760, -4.3105, -5.2323, -4.6200, -4.9103,\n",
      "           -5.0414, -5.0926, -5.0831, -4.6425, -4.0986, -4.7054, -4.6924,\n",
      "           -4.9182, -4.7041, -4.4830, -5.1584, -5.0706, -4.4771, -5.0330,\n",
      "           -4.2649, -4.6336, -4.7563, -5.1932, -4.7054, -5.0218, -4.6650,\n",
      "           -5.1092, -4.2963, -5.2811, -5.1195, -5.0490, -4.9329, -4.6116,\n",
      "           -4.8269, -4.9860, -4.7757, -5.0068, -4.7756, -4.8594, -4.5379,\n",
      "           -4.4097, -5.3564, -5.1726, -5.1785, -4.7006, -4.9684, -4.6883,\n",
      "           -5.1529, -5.1156, -4.8982, -5.3304, -4.7692, -4.9016, -4.8197,\n",
      "           -5.1962, -4.5127, -4.8438, -4.6374, -4.3697, -4.2898, -4.8594,\n",
      "           -5.1724, -4.5691, -5.3479, -4.2359, -5.1356, -5.1747, -5.1559,\n",
      "           -4.8428, -4.7399, -4.9342, -4.8896, -4.3358, -4.8205, -4.9081,\n",
      "           -5.0010, -4.8943, -5.2152, -4.7473, -5.2844, -4.4431, -5.0642,\n",
      "           -4.2915, -5.5020, -4.6199, -4.6222, -4.9117, -4.4481, -5.4661,\n",
      "           -5.1325, -4.3582, -4.4077, -4.6254, -5.1376, -4.9840, -4.9070,\n",
      "           -4.8501, -4.7311, -4.3906, -4.9674, -4.2801, -4.5317, -4.8954,\n",
      "           -4.3692, -4.5688]]]], grad_fn=<LogSoftmaxBackward>), 'buttons': tensor([[[[-9.2849, -8.8826, -8.6615,  ..., -9.1231, -8.9066, -9.1067]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)}\n",
      "words, frmes=1 1\n",
      "VPT1 out shape torch.Size([1, 1, 2048])\n",
      "xattn1 out shape torch.Size([1, 1, 1024])\n",
      "LM out shape torch.Size([1, 1, 1024])\n",
      "LM repeated out shape torch.Size([1, 1, 1024])\n",
      "fused output= torch.Size([1, 1, 2048])\n",
      "single out= False\n",
      "x= torch.Size([1, 1, 2048])\n",
      "{'camera': tensor([[[[-5.1211, -4.5570, -4.6425, -5.2271, -5.0538, -4.6600, -5.0056,\n",
      "           -5.0772, -4.7717, -4.8947, -4.5588, -4.9329, -5.1183, -5.1602,\n",
      "           -4.7041, -4.9308, -4.8347, -4.2931, -4.8652, -4.5369, -4.2168,\n",
      "           -4.5062, -5.1952, -4.7033, -5.2394, -4.9474, -4.9524, -4.8468,\n",
      "           -5.0127, -5.0826, -4.7361, -5.6957, -5.1423, -4.6235, -5.1227,\n",
      "           -5.0009, -4.6125, -4.8219, -5.2606, -5.2877, -4.2209, -4.6162,\n",
      "           -5.0929, -4.4720, -4.7759, -4.6879, -5.2633, -4.9317, -4.8736,\n",
      "           -5.2502, -4.9278, -4.7668, -5.1657, -5.1881, -4.6114, -4.7057,\n",
      "           -4.3555, -4.8541, -4.2866, -4.5922, -4.6572, -5.1957, -4.7125,\n",
      "           -4.6139, -4.5748, -4.8499, -4.4773, -5.0111, -5.4929, -4.5881,\n",
      "           -4.9523, -5.1125, -4.7710, -4.7218, -4.8221, -4.8333, -4.9921,\n",
      "           -5.2363, -4.6843, -4.3110, -4.9173, -4.7618, -5.2343, -5.7275,\n",
      "           -4.6534, -4.7972, -5.2310, -4.4397, -4.3600, -5.2117, -4.5904,\n",
      "           -4.6143, -4.7909, -4.6021, -5.3436, -4.7411, -5.2253, -4.8001,\n",
      "           -5.0704, -5.1683, -4.6051, -4.7593, -4.3424, -4.2428, -4.7893,\n",
      "           -4.7217, -4.5747, -4.9415, -5.1402, -5.0619, -4.7798, -4.6355,\n",
      "           -4.5191, -4.5244, -4.8389, -5.4085, -4.5173, -4.6794, -4.7501,\n",
      "           -5.0092, -4.4307]]]], grad_fn=<LogSoftmaxBackward>), 'buttons': tensor([[[[-9.1806, -9.1310, -8.7503,  ..., -9.0178, -9.1171, -8.7438]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)}\n",
      "words, frmes=2 2\n",
      "VPT1 out shape torch.Size([1, 2, 2048])\n",
      "xattn1 out shape torch.Size([1, 1, 1024])\n",
      "LM out shape torch.Size([1, 1, 1024])\n",
      "LM repeated out shape torch.Size([1, 2, 1024])\n",
      "fused output= torch.Size([1, 2, 2048])\n",
      "single out= False\n",
      "x= torch.Size([1, 2, 2048])\n",
      "{'camera': tensor([[[[-4.8138, -4.9176, -5.1456, -4.7176, -4.8106, -5.0663, -5.2474,\n",
      "           -5.2863, -5.2326, -5.1281, -4.8281, -4.8934, -4.8535, -4.7908,\n",
      "           -4.8386, -5.0008, -4.5760, -4.3105, -5.2323, -4.6200, -4.9103,\n",
      "           -5.0414, -5.0926, -5.0831, -4.6425, -4.0986, -4.7054, -4.6924,\n",
      "           -4.9182, -4.7041, -4.4830, -5.1584, -5.0706, -4.4771, -5.0330,\n",
      "           -4.2649, -4.6336, -4.7563, -5.1932, -4.7054, -5.0218, -4.6650,\n",
      "           -5.1092, -4.2963, -5.2811, -5.1195, -5.0490, -4.9329, -4.6116,\n",
      "           -4.8269, -4.9860, -4.7757, -5.0068, -4.7756, -4.8594, -4.5379,\n",
      "           -4.4097, -5.3564, -5.1726, -5.1785, -4.7006, -4.9684, -4.6883,\n",
      "           -5.1529, -5.1156, -4.8982, -5.3304, -4.7692, -4.9016, -4.8197,\n",
      "           -5.1962, -4.5127, -4.8438, -4.6374, -4.3697, -4.2898, -4.8594,\n",
      "           -5.1724, -4.5691, -5.3479, -4.2359, -5.1356, -5.1747, -5.1559,\n",
      "           -4.8428, -4.7399, -4.9342, -4.8896, -4.3358, -4.8205, -4.9081,\n",
      "           -5.0010, -4.8943, -5.2152, -4.7473, -5.2844, -4.4431, -5.0642,\n",
      "           -4.2915, -5.5020, -4.6199, -4.6222, -4.9117, -4.4481, -5.4661,\n",
      "           -5.1325, -4.3582, -4.4077, -4.6254, -5.1376, -4.9840, -4.9070,\n",
      "           -4.8501, -4.7311, -4.3906, -4.9674, -4.2801, -4.5317, -4.8954,\n",
      "           -4.3692, -4.5688]],\n",
      "\n",
      "         [[-5.1211, -4.5570, -4.6425, -5.2271, -5.0538, -4.6600, -5.0056,\n",
      "           -5.0772, -4.7717, -4.8947, -4.5588, -4.9329, -5.1183, -5.1602,\n",
      "           -4.7041, -4.9308, -4.8347, -4.2931, -4.8652, -4.5369, -4.2168,\n",
      "           -4.5062, -5.1952, -4.7033, -5.2394, -4.9474, -4.9524, -4.8468,\n",
      "           -5.0127, -5.0826, -4.7361, -5.6957, -5.1423, -4.6235, -5.1227,\n",
      "           -5.0009, -4.6125, -4.8219, -5.2606, -5.2877, -4.2210, -4.6162,\n",
      "           -5.0929, -4.4721, -4.7759, -4.6879, -5.2633, -4.9317, -4.8736,\n",
      "           -5.2502, -4.9278, -4.7668, -5.1657, -5.1881, -4.6114, -4.7057,\n",
      "           -4.3555, -4.8541, -4.2866, -4.5922, -4.6572, -5.1957, -4.7125,\n",
      "           -4.6139, -4.5748, -4.8499, -4.4773, -5.0111, -5.4929, -4.5881,\n",
      "           -4.9523, -5.1125, -4.7710, -4.7218, -4.8221, -4.8333, -4.9921,\n",
      "           -5.2363, -4.6843, -4.3110, -4.9173, -4.7618, -5.2343, -5.7275,\n",
      "           -4.6534, -4.7972, -5.2310, -4.4397, -4.3600, -5.2117, -4.5904,\n",
      "           -4.6143, -4.7909, -4.6021, -5.3436, -4.7411, -5.2253, -4.8001,\n",
      "           -5.0704, -5.1683, -4.6051, -4.7593, -4.3424, -4.2428, -4.7893,\n",
      "           -4.7217, -4.5747, -4.9415, -5.1402, -5.0619, -4.7798, -4.6355,\n",
      "           -4.5191, -4.5244, -4.8389, -5.4085, -4.5173, -4.6794, -4.7501,\n",
      "           -5.0092, -4.4307]]]], grad_fn=<LogSoftmaxBackward>), 'buttons': tensor([[[[-9.2849, -8.8826, -8.6615,  ..., -9.1231, -8.9066, -9.1067]],\n",
      "\n",
      "         [[-9.1806, -9.1310, -8.7503,  ..., -9.0178, -9.1171, -8.7438]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)}\n"
     ]
    }
   ],
   "source": [
    "frameA = th.normal(0,1,[1,1,128,128,3])\n",
    "frameB = th.normal(0,1,[1,1,128,128,3])\n",
    "tokenA = th.full([1,1],2, dtype=th.long)\n",
    "tokenB = th.full([1,1],4127, dtype=th.long)\n",
    "\n",
    "# get output for input P(A)\n",
    "frames = {'img':frameA,\n",
    "            'ms': th.tensor([0,0])}\n",
    "\n",
    "words = {'token_ids':tokenA,\n",
    "            'ms': th.tensor([0,0])}\n",
    "pd_action, vpred_action, pd_word, VPT_state_out = agent.policy.get_output_for_observations(ob_frames=frames, ob_words=words)\n",
    "print(pd_action)\n",
    "\n",
    "\n",
    "\n",
    "# get output for input P(A)->state->P(B)\n",
    "frames = {'img':frameB,\n",
    "            'ms': th.tensor([0,0])}\n",
    "\n",
    "words = {'token_ids':tokenB,\n",
    "            'ms': th.tensor([0,0])}\n",
    "pd_action, vpred_action, pd_word, VPT_state_out = agent.policy.get_output_for_observations(ob_frames=frames, ob_words=words, VPT_state_in=VPT_state_out)\n",
    "print(pd_action)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get output for P(A,B)\n",
    "frames = {'img':th.cat([frameA,frameB], axis=1),\n",
    "            'ms': th.tensor([0,0])}\n",
    "\n",
    "words = {'token_ids':th.cat([tokenA,tokenB], axis=1),\n",
    "            'ms': th.tensor([0,0])}\n",
    "pd_action, vpred_action, pd_word, VPT_state_out = agent.policy.get_output_for_observations(ob_frames=frames, ob_words=words)\n",
    "print(pd_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False, False, False, False, False,\n",
      "          False, False, False, False, False, False,  True,  True]]]), (tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1803,  0.0693,  0.1061,  ...,  0.2710,  0.1375,  0.1685],\n",
      "         [ 0.1200,  0.0113, -0.2332,  ...,  0.0011,  0.1919,  0.3061]]],\n",
      "       grad_fn=<SliceBackward>), tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3404, -0.1877, -0.3894,  ...,  0.9974, -0.6729, -0.5019],\n",
      "         [ 0.5116, -0.8037,  0.0591,  ...,  0.1663, -0.7813, -0.5542]]],\n",
      "       grad_fn=<SliceBackward>)))\n"
     ]
    }
   ],
   "source": [
    "print(VPT_state_out[0])#agent.policy.initial_state(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.arange(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from inverse_dynamics_model import IDMAgent\n",
    "\n",
    "agent_parameters = pickle.load(open('4x_idm.model', \"rb\"))\n",
    "net_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "agent = IDMAgent(device=DEVICE, idm_net_kwargs=net_kwargs, pi_head_kwargs=pi_head_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = th.from_numpy(np.array((False,))).to(DEVICE)\n",
    "state = policy.initial_state(1)\n",
    "print(len(state))\n",
    "\n",
    "ob_img = {\"pov\": np.random.normal(0,0.02,(128,128,3)) }\n",
    "ob_img = agent._env_obs_to_agent( ob_img )\n",
    "print(\"ob_img shape:\",ob_img[\"img\"].shape)\n",
    "\n",
    "ob_words = agent._words_to_agent(\"hi, this is a test sentence ... test1 ... hello there ... test 3.\")\n",
    "print(ob_words['token_ids'].shape)\n",
    "\n",
    "\n",
    "\n",
    "action_prob, action, pd_word, state_out = policy.get_output_for_observation(ob_words=ob_words, ob_img=ob_img, first=first, VPT_state_in=state) # test for frames=1 and n_frames=2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ob_frames = [np.ones((128,128,3))*0.01 for i in range(10)]\n",
    "ob_frames = agent._video_obs_to_agent( ob_frames )\n",
    "ob_frames['img'][0,:,:,:,:] = th.ones([10,128,128,3])*-0.05\n",
    "ob_words = agent._words_to_agent(\"I'm going to count to 10: 1, 2, 3, 4, 5, 6, 7, 8, \")\n",
    "\n",
    "policy.net.Xattn_VPT_LM.alpha_xattn=th.nn.Parameter(th.tensor(0.))\n",
    "policy.net.Xattn_VPT_LM.alpha_xattn=th.nn.Parameter(th.tensor(0.))\n",
    "\n",
    "action_prob, action, pd_word = policy.get_output_for_observations(ob_words=ob_words, ob_frames=ob_frames) # test for frames=1 and n_frames=2\n",
    "print(pd_word.mean(), pd_word.std())\n",
    "\n",
    "\n",
    "token_index = th.argmax(pd_word[0,-1,:]).item()\n",
    "\n",
    "print(agent._agent_words_to_string(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVhoKnqVM5Fd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 8\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 12\n",
    "LOSS_REPORT_RATE = 100\n",
    "LEARNING_RATE = 0.000181\n",
    "WEIGHT_DECAY = 0.039428\n",
    "MAX_GRAD_NORM = 5.0\n",
    "# Basic behavioural cloning\n",
    "# Note: this uses gradient accumulation in batches of ones\n",
    "#       to perform training.\n",
    "#       This will fit inside even smaller GPUs (tested on 8GB one),\n",
    "#       but is slow.\n",
    "# NOTE: This is _not_ the original code used for VPT!\n",
    "#       This is merely to illustrate how to fine-tune the models and includes\n",
    "#       the processing steps used.                                               @FIX THIS to run on 4x3090 = 96GB:\n",
    "\n",
    "# This will likely be much worse than what original VPT did:\n",
    "# we are not training on full sequences, but only one step at a time to save VRAM.\n",
    "def behavioural_cloning_train(data_dir, in_model, in_weights, out_weights):\n",
    "    agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n",
    "\n",
    "    # To create model with the right environment.\n",
    "    # All basalt environments have the same settings, so any of them works here\n",
    "    env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
    "    agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n",
    "    agent.load_weights(in_weights)\n",
    "    env.close()\n",
    "\n",
    "    policy = agent.policy\n",
    "    trainable_parameters = policy.parameters()\n",
    "\n",
    "    # Parameters taken from the OpenAI VPT paper\n",
    "    optimizer = th.optim.Adam(\n",
    "        trainable_parameters,\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset_dir=data_dir,\n",
    "        n_workers=N_WORKERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Keep track of the hidden state per episode/trajectory.\n",
    "    # DataLoader provides unique id for each episode, which will\n",
    "    # be different even for the same trajectory when it is loaded\n",
    "    # up again\n",
    "    episode_hidden_states = {}\n",
    "    dummy_first = th.from_numpy(np.array((False,))).to(DEVICE)\n",
    "\n",
    "    loss_sum = 0\n",
    "    for batch_i, (batch_images, batch_actions, batch_episode_id) in enumerate(data_loader):\n",
    "        batch_loss = 0\n",
    "        for image, action, episode_id in zip(batch_images, batch_actions, batch_episode_id):\n",
    "            \n",
    "            agent_action = agent._env_action_to_agent(action, to_torch=True, check_if_null=True)\n",
    "            \n",
    "            if agent_action is None:\n",
    "                # Action was null\n",
    "                continue\n",
    "\n",
    "            agent_obs = agent._video_obs_to_agent({\"pov\": image})\n",
    "\n",
    "            if episode_id not in episode_hidden_states:\n",
    "                # TODO need to clean up this hidden state after worker is done with the work item.\n",
    "                #      Leaks memory, but not tooooo much at these scales (will be a problem later).\n",
    "                episode_hidden_states[episode_id] = policy.initial_state(1)\n",
    "            agent_state = episode_hidden_states[episode_id]\n",
    "            \n",
    "            pi_distribution, v_prediction, new_agent_state, word_pd, word_v = policy.get_output_for_observations(\n",
    "                ob_frames = agent_obs,\n",
    "                ob_words = ob_words,\n",
    "                VPT_state_in = agent_state,\n",
    "                dummy_first = None\n",
    "            )\n",
    "\n",
    "            if (LM_full):\n",
    "                loss = \n",
    "\n",
    "\n",
    "            # ACTION LOSS\n",
    "            log_prob  = policy.get_logprob_of_action(pi_distribution, agent_action)\n",
    "\n",
    "\n",
    "            # LANGUAGE LOSS - from modeling_opt.py\n",
    "            logits = word_pd.contiguous()\n",
    "            labels = th.full([word_pd.shape[0],word_pd.shape[1],1], -100, dtype=th.LongTensor) # dont compute loss for masked tokens (hence -100 token ids for masked tokens, as per OPT)\n",
    "            for b in range(len(ob_words['input_ids'])):\n",
    "                n_tokens = len(ob_words['input_ids'][b])\n",
    "                labels[:n_tokens] = ob_words['input_ids']\n",
    "            loss = None\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()      #@ make sure to not train \n",
    "            # Flatten the tokens\n",
    "            loss_fct = th.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, policy.net.LM.model.decoder.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Make sure we do not try to backprop through sequence\n",
    "            # (fails with current accumulation)\n",
    "            new_agent_state = tree_map(lambda x: x.detach(), new_agent_state)\n",
    "            episode_hidden_states[episode_id] = new_agent_state\n",
    "\n",
    "            # Finally, update the agent to increase the probability of the\n",
    "            # taken action.\n",
    "            # Remember to take mean over batch losses\n",
    "            loss = -log_prob / BATCH_SIZE\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "        th.nn.utils.clip_grad_norm_(trainable_parameters, MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += batch_loss\n",
    "        if batch_i % LOSS_REPORT_RATE == 0:\n",
    "            time_since_start = time.time() - start_time\n",
    "            print(f\"Time: {time_since_start:.2f}, Batches: {batch_i}, Avrg loss: {loss_sum / LOSS_REPORT_RATE:.4f}\")\n",
    "            loss_sum = 0\n",
    "\n",
    "    state_dict = policy.state_dict()\n",
    "    th.save(state_dict, out_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "attention_heads': 16,\n",
    "  'attention_mask_style': 'clipped_causal',\n",
    "  'attention_memory_size': 256,\n",
    "  'diff_mlp_embedding': False,\n",
    "  'hidsize': 2048,\n",
    "  'img_shape': [128, 128, 3],\n",
    "  'impala_chans': [16, 32, 32],\n",
    "  'impala_kwargs': {'post_pool_groups': 1},\n",
    "  'impala_width': 8,\n",
    "  'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1},\n",
    "  'n_recurrence_layers': 4,\n",
    "  'only_img_input': True,\n",
    "  'pointwise_ratio': 4,\n",
    "  'pointwise_use_activation': False,\n",
    "  'recurrence_is_residual': True,\n",
    "  'recurrence_type': 'transformer',\n",
    "  'timesteps': 128,\n",
    "  'use_pointwise_layer': True,\n",
    "  'use_pre_lstm_ln': False},\n",
    "\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = th.nn.MultiheadAttention(\n",
    "            embed_dim=512,  # in pytorch, its split across heads\n",
    "            num_heads=16,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            add_zero_attn=False,\n",
    "            kdim=128*16,vdim=128*16,\n",
    "            batch_first=True,\n",
    "            device=DEVICE,\n",
    "            dtype=th.float32)\n",
    "\n",
    "tokensL = th.normal(0,0.02,[2,4,512])\n",
    "tokensV = th.normal(0,0.02,[2,5,2048])\n",
    "\n",
    "attn_mask = th.zeros([4,5])\n",
    "attn_mask[-1,:] = 1\n",
    "print(attn_mask)\n",
    "\n",
    "out = attention(query=tokensL, key=tokensV, value=tokensV, attn_mask=attn_mask)\n",
    "print(out)\n",
    "th.isnan(out[0]).any()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMW0NSERlVQQUPphW+YKITN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('minerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5771c508f711f7da473bf0954744d30385b19c5daa239986e839c4dfdaff2669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
