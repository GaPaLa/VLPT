{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1669722448788,
     "user": {
      "displayName": "Idmi",
      "userId": "02220947904692487762"
     },
     "user_tz": 0
    },
    "id": "nbezCFQUbal_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CPU\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys, os\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "#from data_loader import DataLoader\n",
    "from lib.tree_util import tree_map\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    DEVICE = th.device('cuda')\n",
    "    print(\"USING CUDA\")\n",
    "else:\n",
    "\n",
    "    DEVICE = th.device('cpu')\n",
    "    print(\"USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([240])\n",
      "1 torch.Size([240])\n",
      "2 torch.Size([48, 5])\n",
      "3 torch.Size([48, 5])\n",
      "final 12\n",
      "{'input_ids': tensor([[ 6415,     2,   222,   617,   237,    28,  2452,     2,    68,    61,\n",
      "          4867,    38,    35, 27875,    80,    29,   138,   220,     3,   147,\n",
      "            29,  3024,     2,    68,  2271,    38,  1302,  2452,     2,     5,\n",
      "           138,   767,    29,  7116, 28661,     6, 10966,   304,     3]])}\n",
      "Well, if my name were Daniel, I would certainly not be joking about it so much. As it stands, I am not names Daniel, and so find it perfectly apt to mock you.\n",
      "<eos>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m#LOAD LM\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TransfoXLLMHeadModel\n\u001b[0;32m---> 64\u001b[0m model \u001b[39m=\u001b[39m TransfoXLLMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mtransfo-xl-wt103\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39md_embed)\n\u001b[1;32m     68\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mn_layer)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/transformers/modeling_utils.py:2012\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2011\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2012\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   2013\u001b[0m         config_path,\n\u001b[1;32m   2014\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2015\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2016\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2017\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2018\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2019\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2020\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2021\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2022\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2023\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   2024\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   2025\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2026\u001b[0m     )\n\u001b[1;32m   2027\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2028\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/transformers/configuration_utils.py:532\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    534\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    535\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    561\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    612\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    615\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    616\u001b[0m         configuration_file,\n\u001b[1;32m    617\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    618\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    619\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    620\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    621\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    622\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    623\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    624\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    625\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    626\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    627\u001b[0m     )\n\u001b[1;32m    628\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    629\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/file_download.py:1067\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1066\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1068\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1069\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1070\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1071\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1074\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1076\u001b[0m             HUGGINGFACE_HEADER_X_REPO_COMMIT\n\u001b[1;32m   1077\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1364\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(token\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m   1366\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1368\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1369\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1370\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1371\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1372\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1373\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1374\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1375\u001b[0m )\n\u001b[1;32m   1376\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1378\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/file_download.py:403\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 403\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    404\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    405\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    406\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    407\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    408\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    409\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    410\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    411\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    414\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/file_download.py:438\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    437\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    439\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    440\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    441\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    442\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    443\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    444\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(ConnectTimeout, ProxyError),\n\u001b[1;32m    445\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    446\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    447\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    448\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:129\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    128\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerl/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"\"\"= = Distribution = =\n",
    "Species range across the Neotropics from Mexico in the north to Bolivia, Paraguay, and southern Brazil in the south. According to <unk> and coauthors, three\n",
    "species are found in Mexico, four in Central America, and 62 in South America. Three species are present in the Caribbean â€” two in Trinidad and Tobago, along\n",
    "the southern edge of the region, and one in Haiti.\"\"\"\n",
    "import numpy as np\n",
    "path = 'text.txt'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if True and idx > 0 and idx % 500000 == 0:\n",
    "            pass\n",
    "            #print('    line {}'.format(idx))\n",
    "        #print('LINE',line)\n",
    "\n",
    "\n",
    "data = th.LongTensor(np.arange(5*16*3))\n",
    "\n",
    "bsz = 5\n",
    "bptt = 4\n",
    "ext_len = 0\n",
    "\n",
    "# Work out how cleanly we can divide the dataset into bsz parts.\n",
    "print('0',data.shape)\n",
    "n_step = data.size(0) // bsz\n",
    "\n",
    "# Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "data = data.narrow(0, 0, n_step * bsz)\n",
    "print('1',data.shape)\n",
    "\n",
    "\n",
    "# Evenly divide the data across the bsz batches.\n",
    "data = data.view(bsz, -1).t().contiguous()\n",
    "print('2',data.shape)\n",
    "\n",
    "# Number of mini-batches\n",
    "n_batch = (n_step + bptt - 1) // bptt\n",
    "print('3',data.shape)\n",
    "\n",
    "print('final', n_batch)\n",
    "\n",
    "# LOAD TOKENIZER\n",
    "from transformers import TransfoXLTokenizer\n",
    "text = \"Well, if my name were Daniel, I would certainly not be joking about it so much. As it stands, I am not names Daniel, and so find it perfectly apt to mock you.\"\n",
    "\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")\n",
    "## TEST TOKENIZER\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "n_tokens = inputs['input_ids'].shape[1]\n",
    "print(inputs)\n",
    "inputs_list = inputs['input_ids'].reshape([n_tokens]).tolist()\n",
    "print(tokenizer.decode(inputs_list))\n",
    "print(tokenizer.decode([0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LOAD LM\n",
    "from transformers import TransfoXLLMHeadModel\n",
    "model = TransfoXLLMHeadModel.from_pretrained(\"transfo-xl-wt103\")\n",
    "\n",
    "\n",
    "print(model.transformer.d_embed)\n",
    "print(model.transformer.n_layer)\n",
    "print(model.transformer.n_head)\n",
    "print(model.transformer.d_model)\n",
    "print(model.transformer.config.bos_token_id)\n",
    "print(model.transformer.config.vocab_size)\n",
    "print(model.sample_softmax)\n",
    "print(model.transformer.config.div_val)\n",
    "print(model.transformer.word_emb)\n",
    "print(model.transformer.word_emb)\n",
    "\n",
    "print(model.config.torchscript)\n",
    "print(model.trainer_compatible)\n",
    "\n",
    "\n",
    "print(model.num_parameters())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#outputs = model(inputs_embeds=torch.normal(0,1,[1,5,1024]))\n",
    "input_ids = th.tensor([[6415,     2,   222,   617,   237,    28,  2452,     2,    68,    61,\n",
    "          4867,    38,    35, 27875,    80,    29,   138,   220,     3,   147,\n",
    "            29,  3024,     2,    68,  2271,    38,  1302,  2452,     2,     5,\n",
    "           138,   767,    29,  7116, 28661,     6, 10966,   304,     3]],\n",
    "            dtype=th.long)\n",
    "\n",
    "outputs = model(input_ids=input_ids, labels=input_ids.roll(1, dims=1))\n",
    "#outputs2 = model(input_ids=input_ids, labels=input_ids)\n",
    "\n",
    "#\n",
    "\n",
    "#print(tokenizer.decode(outputs.prediction_scores[0,5,1000]))\n",
    "\n",
    "\n",
    "\n",
    "#print(outputs)\n",
    "print(outputs.logits)\n",
    "print(outputs.loss)\n",
    "\n",
    "\n",
    "maxes = th.argmax(outputs.logits, axis=1)\n",
    "print(input_ids.shape)\n",
    "print(maxes.shape)\n",
    "print(tokenizer.decode(input_ids[0]),\"\\n\")\n",
    "print(tokenizer.decode(maxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1669722218180,
     "user": {
      "displayName": "Idmi",
      "userId": "02220947904692487762"
     },
     "user_tz": 0
    },
    "id": "Wfv11S7AjPbo",
    "outputId": "dea05d6d-d32f-4dbd-fbf1-398affc96078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: USING CUSTOM TRANSFORMER_XL UTILITIES.py. DO NOT USE THE LMHEAD.FORWAD() METHOD IWTH LABELS, IT WILL GIVE INCORRECT LOSS AND PREDICTIONS\n",
      "2048\n",
      "{'active_reward_monitors': {'craft_stats': {'args': {'collapse_var': True, 'items': ['planks', 'stick', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe', 'furnace', 'iron_ingot', 'iron_pickaxe', 'diamond_pickaxe', 'torch']}, 'weight': 0}, 'mine_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal_ore', 'stone', 'iron_ore', 'diamond_ore', 'obsidian']}, 'weight': 0}, 'order_invariant_curriculum': {'args': {'curriculum': {'coal': [5, 0.4], 'cobblestone': [11, 0.09090909090909091], 'crafting_table': [1, 1], 'diamond': [10000, 2.6666666666666665], 'diamond_pickaxe': [10000, 8], 'furnace': [1, 1], 'iron_ingot': [3, 1.3333333333333333], 'iron_ore': [3, 1.3333333333333333], 'iron_pickaxe': [1, 4], 'log': [8, 0.125], 'obsidian': [10000, 16], 'planks': [20, 0.05], 'stick': [16, 0.0625], 'stone_pickaxe': [1, 1], 'torch': [16, 0.125], 'wooden_pickaxe': [1, 1]}}, 'weight': 1}, 'pickup_stats': {'args': {'collapse_var': True, 'items': ['log', 'coal', 'cobblestone', 'iron_ore', 'diamond']}, 'weight': 0}, 'variety': {'args': {'collapse_var': True, 'included_items': ['beef', 'chicken', 'leather', 'mutton', 'porkchop', 'bucket', 'milk_bucket', 'water_bucket', 'coal', 'crafting_table', 'furnace', 'diamond', 'gold_ingot', 'gold_ore', 'flint', 'iron_ingot', 'iron_ore', 'shears', 'string', 'cobblestone', 'log', 'planks', 'stick', 'wool', 'obsidian', 'paper', 'redstone', 'wheat', 'cooked_beef', 'cooked_chicken', 'cooked_mutton', 'cooked_porkchop', 'egg', 'feather', 'leather_boots', 'leather_chestplate', 'leather_helmet', 'leather_leggings', 'brick', 'brick_stairs', 'clay', 'clay_ball', 'flower_pot', 'terracotta', 'torch', 'diamond_axe', 'diamond_block', 'diamond_boots', 'diamond_chestplate', 'diamond_helmet', 'diamond_hoe', 'diamond_leggings', 'diamond_pickaxe', 'diamond_shovel', 'diamond_sword', 'dirt', 'golden_apple', 'golden_axe', 'golden_boots', 'golden_chestplate', 'golden_helmet', 'golden_hoe', 'golden_leggings', 'golden_pickaxe', 'golden_shovel', 'golden_sword', 'arrow', 'gravel', 'iron_axe', 'iron_boots', 'iron_chestplate', 'iron_helmet', 'iron_hoe', 'iron_leggings', 'iron_pickaxe', 'iron_shovel', 'iron_sword', 'shield', 'fermented_spider_eye', 'leaves', 'apple', 'bread', 'activator_rail', 'clock', 'compass', 'detector_rail', 'dropper', 'book', 'bookshelf', 'cake', 'filled_map', 'sugar_cane', 'sugar', 'bow', 'dispenser', 'fishing_rod', 'spider_eye', 'stone_axe', 'stone_hoe', 'stone_pickaxe', 'stone_shovel', 'stone_sword', 'boat', 'wooden_axe', 'wooden_hoe', 'wooden_pickaxe', 'wooden_shovel', 'wooden_sword', 'banner', 'bed', 'carpet', 'map', 'redstone_torch', 'wheat_seeds', 'flint_and_steel']}, 'weight': 0}}, 'attention_heads': 16, 'attention_mask_style': 'clipped_causal', 'attention_memory_size': 256, 'diff_mlp_embedding': False, 'hidsize': 2048, 'img_shape': [128, 128, 3], 'impala_chans': [16, 32, 32], 'impala_kwargs': {'post_pool_groups': 1}, 'impala_width': 8, 'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1}, 'n_recurrence_layers': 4, 'only_img_input': True, 'pointwise_ratio': 4, 'pointwise_use_activation': False, 'recurrence_is_residual': True, 'recurrence_type': 'transformer', 'timesteps': 128, 'use_pointwise_layer': True, 'use_pre_lstm_ln': False}\n"
     ]
    }
   ],
   "source": [
    "def load_model_parameters(path_to_model_file):\n",
    "    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n",
    "    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "\n",
    "    return policy_kwargs, pi_head_kwargs\n",
    "\n",
    "from agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "\n",
    "agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(\"2x.model\")\n",
    "\n",
    "env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
    "agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n",
    "env.close()\n",
    "policy = agent.policy\n",
    "trainable_parameters = policy.parameters()\n",
    "\n",
    "print(policy.net.hidsize)\n",
    "print(agent_policy_kwargs)\n",
    "\n",
    "\n",
    "#frames = agent._video_obs_to_agent(frames)\n",
    "#print(agent.predict_actions(np.repeat(frames,1,axis=0)))\n",
    "#print(agent.hidden_state[0])\n",
    "\n",
    "from agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT1 in shape torch.Size([1, 16, 2048]) torch.Size([1, 16])\n",
      "VPT1 out shape torch.Size([1, 16, 2048])\n",
      "xattn1 out shape torch.Size([1, 8, 1024])\n",
      "LM words shape torch.Size([1, 8, 267735])\n",
      "fused output= torch.Size([1, 16, 2048])\n",
      "single out= False\n",
      "x= torch.Size([1, 16, 2048])\n",
      "WORDS SHAPE,  torch.Size([1, 8, 267735])\n",
      "{'camera': tensor([[[[-4.5811, -5.0459, -4.8409,  ..., -4.9318, -4.8374, -5.0652]],\n",
      "\n",
      "         [[-4.7805, -4.8480, -4.3274,  ..., -4.7990, -5.2171, -4.9073]],\n",
      "\n",
      "         [[-4.8575, -4.5798, -4.5286,  ..., -4.6043, -4.9489, -4.3944]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0813, -4.7324, -4.8874,  ..., -4.9626, -4.3089, -4.7489]],\n",
      "\n",
      "         [[-4.5478, -4.7749, -4.2579,  ..., -4.5925, -5.1912, -5.0916]],\n",
      "\n",
      "         [[-5.0843, -4.6041, -4.6655,  ..., -5.1816, -5.4980, -4.6445]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>), 'buttons': tensor([[[[-9.5103, -8.7201, -9.0495,  ..., -9.3198, -8.9327, -9.2535]],\n",
      "\n",
      "         [[-9.4636, -8.4413, -9.0567,  ..., -9.6439, -8.7130, -9.3567]],\n",
      "\n",
      "         [[-9.2691, -9.1800, -8.6910,  ..., -9.2260, -9.0389, -9.6551]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.0529, -9.2523, -9.4128,  ..., -9.3447, -9.3944, -9.1151]],\n",
      "\n",
      "         [[-9.3132, -9.0702, -8.9648,  ..., -8.9740, -8.7792, -9.4702]],\n",
      "\n",
      "         [[-9.5065, -9.5372, -8.5457,  ..., -9.1693, -9.0290, -8.7005]]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)}\n"
     ]
    }
   ],
   "source": [
    "# TEST POLICY RECURRENT/TRANSFORMER BEHAVIOUR\n",
    "seq_len = 16\n",
    "bsz = 1\n",
    "frameA = th.normal(0,1,[bsz,seq_len,128,128,3])\n",
    "frameB = th.normal(0,1,[bsz,seq_len,128,128,3])\n",
    "tokenA = th.full([bsz,seq_len],4127, dtype=th.long)\n",
    "tokenB = th.full([bsz,seq_len],3467, dtype=th.long)\n",
    "\n",
    "# get output for input P(A)\n",
    "frames = {'img':frameA,\n",
    "            'ms': th.zeros([bsz,seq_len])}\n",
    "\n",
    "words = {'token_ids':tokenA,\n",
    "            'ms': th.zeros([bsz,seq_len])}\n",
    "#with th.no_grad():\n",
    "pi_latent, vf_latent, LM_words, VPT_state_out, LM_state_out, LM_loss = agent.policy.get_output_for_observations(ob_frames=frames, ob_words=words, LM_active_timestep=True)\n",
    "print(pi_latent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name            size_Mb    id\n",
      "MineRLAgent        0.0014  111786560\n",
      "Dataset            0.0010  66711920\n",
      "_i2                0.0010  62211056\n",
      "Tensor             0.0010  60408832\n",
      "ArgumentParser     0.0010  12848128\n",
      "_i                 0.0007  1325456288\n",
      "_i7                0.0007  1325456288\n",
      "_ii                0.0007  301605680\n",
      "_i6                0.0007  301605680\n",
      "_i8                0.0007  234499056\n"
     ]
    }
   ],
   "source": [
    "def lsos(all_obj = globals(),n=10):\n",
    "\n",
    "    import sys\n",
    "\n",
    "    object_name = list(all_obj)\n",
    "    object_size = [ round(sys.getsizeof(all_obj[x])/1024.0/1024.0,4) for x in object_name]\n",
    "    object_id = [id(all_obj[x]) for x in object_name]\n",
    "\n",
    "    d = [(a,b,c) for a,b,c in zip(object_name, object_size, object_id)]\n",
    "    d.sort(key = lambda x:(x[1],x[2]), reverse=True)\n",
    "    dprint = d[0:min(len(d), n)]\n",
    "\n",
    "    #print formating\n",
    "    name_width_max = max([len(x[0]) for x in dprint])\n",
    "    print((\"{:<\" + str(name_width_max +2) + \"}{:11}{}\").format(\"name\",\"size_Mb\",\"id\"))\n",
    "    fmt = '{{:<{}}}'.format(name_width_max+2) +\"  \"+ \"{: 5.4f}\" +\"  \"+ \"{:d}\"\n",
    "    for line in dprint:\n",
    "        print( fmt.format(*line))\n",
    "\n",
    "    return\n",
    "\n",
    "lsos(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   16, 12963],\n",
       "        [   16, 12963],\n",
       "        [   16, 12963]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_words.shape\n",
    "th.argmax(LM_words, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. x shape: torch.Size([2, 1, 128, 128, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idmi/miniconda3/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. x shape: torch.Size([2, 1, 4096])\n",
      "shape of IDM predict output: {'buttons': tensor([[[0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]]]), 'camera': tensor([[[3, 2]],\n",
      "\n",
      "        [[5, 4]]])}\n",
      "hidden state: [(None, (tensor([], size=(2, 0, 4096)), tensor([], size=(2, 0, 4096)))), (None, (tensor([], size=(2, 0, 4096)), tensor([], size=(2, 0, 4096))))]\n"
     ]
    }
   ],
   "source": [
    "# TEST IDM BEHAVIOUR\n",
    "\n",
    "from inverse_dynamics_model import IDMAgent\n",
    "\n",
    "agent_parameters = pickle.load(open('4x_idm.model', \"rb\"))\n",
    "net_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "agent = IDMAgent(device=DEVICE, idm_net_kwargs=net_kwargs, pi_head_kwargs=pi_head_kwargs)\n",
    "\n",
    "first = th.from_numpy(np.array((False,))).to(DEVICE)\n",
    "\n",
    "#frames = [np.random.normal(0,0.02,(128,128,3)),  np.random.normal(0,0.02,(128,128,3)),  np.random.normal(0,0.02,(128,128,3))]\n",
    "\n",
    "frames = np.random.normal(0,0.02,(2,1,128,128,3))\n",
    "ac = agent.predict_actions(frames, raw_frames=True) # test for frames=1 and n_frames=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(ac[\u001b[39m'\u001b[39m\u001b[39mcamera\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ac' is not defined"
     ]
    }
   ],
   "source": [
    "print(ac['camera'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TEST X-ATTN\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ob_frames \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mones((\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m3\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m0.01\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m ob_frames \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39m_video_obs_to_agent( ob_frames )\n\u001b[1;32m      5\u001b[0m ob_frames[\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m,:,:,:,:] \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones([\u001b[39m10\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m3\u001b[39m])\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.05\u001b[39m\n\u001b[1;32m      6\u001b[0m ob_words \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39m_words_to_agent(\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm going to count to 10: 1, 2, 3, 4, 5, 6, 7, 8, \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# TEST X-ATTN\n",
    "\n",
    "ob_frames = [np.ones((128,128,3))*0.01 for i in range(10)]\n",
    "ob_frames = agent._video_obs_to_agent( ob_frames )\n",
    "ob_frames['img'][0,:,:,:,:] = th.ones([10,128,128,3])*-0.05\n",
    "ob_words = agent._words_to_agent(\"I'm going to count to 10: 1, 2, 3, 4, 5, 6, 7, 8, \")\n",
    "\n",
    "policy.net.Xattn_VPT_LM.alpha_xattn=th.nn.Parameter(th.tensor(0.))\n",
    "policy.net.Xattn_VPT_LM.alpha_xattn=th.nn.Parameter(th.tensor(0.))\n",
    "\n",
    "action_prob, action, pd_word = policy.get_output_for_observations(ob_words=ob_words, ob_frames=ob_frames) # test for frames=1 and n_frames=2\n",
    "print(pd_word.mean(), pd_word.std())\n",
    "\n",
    "\n",
    "token_index = th.argmax(pd_word[0,-1,:]).item()\n",
    "\n",
    "print(agent._agent_words_to_string(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVhoKnqVM5Fd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 8\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 12\n",
    "LOSS_REPORT_RATE = 100\n",
    "LEARNING_RATE = 0.000181\n",
    "WEIGHT_DECAY = 0.039428\n",
    "MAX_GRAD_NORM = 5.0\n",
    "# Basic behavioural cloning\n",
    "# Note: this uses gradient accumulation in batches of ones\n",
    "#       to perform training.\n",
    "#       This will fit inside even smaller GPUs (tested on 8GB one),\n",
    "#       but is slow.\n",
    "# NOTE: This is _not_ the original code used for VPT!\n",
    "#       This is merely to illustrate how to fine-tune the models and includes\n",
    "#       the processing steps used.                                               @FIX THIS to run on 4x3090 = 96GB:\n",
    "\n",
    "# This will likely be much worse than what original VPT did:\n",
    "# we are not training on full sequences, but only one step at a time to save VRAM.\n",
    "def behavioural_cloning_train(data_dir, in_model, in_weights, out_weights):\n",
    "    agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n",
    "\n",
    "    # To create model with the right environment.\n",
    "    # All basalt environments have the same settings, so any of them works here\n",
    "    env = gym.make(\"MineRLBasaltFindCave-v0\")\n",
    "    agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n",
    "    agent.load_weights(in_weights)\n",
    "    env.close()\n",
    "\n",
    "    policy = agent.policy\n",
    "    trainable_parameters = policy.parameters()\n",
    "\n",
    "    # Parameters taken from the OpenAI VPT paper\n",
    "    optimizer = th.optim.Adam(\n",
    "        trainable_parameters,\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset_dir=data_dir,\n",
    "        n_workers=N_WORKERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Keep track of the hidden state per episode/trajectory.\n",
    "    # DataLoader provides unique id for each episode, which will\n",
    "    # be different even for the same trajectory when it is loaded\n",
    "    # up again\n",
    "    episode_hidden_states = {}\n",
    "    dummy_first = th.from_numpy(np.array((False,))).to(DEVICE)\n",
    "\n",
    "    loss_sum = 0\n",
    "    for batch_i, (batch_images, batch_actions, batch_episode_id) in enumerate(data_loader):\n",
    "        batch_loss = 0\n",
    "        for image, action, episode_id in zip(batch_images, batch_actions, batch_episode_id):\n",
    "            \n",
    "            agent_action = agent._env_action_to_agent(action, to_torch=True, check_if_null=True)\n",
    "            \n",
    "            if agent_action is None:\n",
    "                # Action was null\n",
    "                continue\n",
    "\n",
    "            agent_obs = agent._video_obs_to_agent({\"pov\": image})\n",
    "\n",
    "            if episode_id not in episode_hidden_states:\n",
    "                # TODO need to clean up this hidden state after worker is done with the work item.\n",
    "                #      Leaks memory, but not tooooo much at these scales (will be a problem later).\n",
    "                episode_hidden_states[episode_id] = policy.initial_state(1)\n",
    "            agent_state = episode_hidden_states[episode_id]\n",
    "            \n",
    "            pi_distribution, v_prediction, new_agent_state, word_pd, word_v = policy.get_output_for_observations(\n",
    "                ob_frames = agent_obs,\n",
    "                ob_words = ob_words,\n",
    "                VPT_state_in = agent_state,\n",
    "                dummy_first = None\n",
    "            )\n",
    "\n",
    "            if (LM_full):\n",
    "                loss = \n",
    "\n",
    "\n",
    "            # ACTION LOSS\n",
    "            log_prob  = policy.get_logprob_of_action(pi_distribution, agent_action)\n",
    "\n",
    "\n",
    "            # LANGUAGE LOSS - from modeling_opt.py\n",
    "            logits = word_pd.contiguous()\n",
    "            labels = th.full([word_pd.shape[0],word_pd.shape[1],1], -100, dtype=th.LongTensor) # dont compute loss for masked tokens (hence -100 token ids for masked tokens, as per OPT)\n",
    "            for b in range(len(ob_words['input_ids'])):\n",
    "                n_tokens = len(ob_words['input_ids'][b])\n",
    "                labels[:n_tokens] = ob_words['input_ids']\n",
    "            loss = None\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()      #@ make sure to not train \n",
    "            # Flatten the tokens\n",
    "            loss_fct = th.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, policy.net.LM.model.decoder.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Make sure we do not try to backprop through sequence\n",
    "            # (fails with current accumulation)\n",
    "            new_agent_state = tree_map(lambda x: x.detach(), new_agent_state)\n",
    "            episode_hidden_states[episode_id] = new_agent_state\n",
    "\n",
    "            # Finally, update the agent to increase the probability of the\n",
    "            # taken action.\n",
    "            # Remember to take mean over batch losses\n",
    "            loss = -log_prob / BATCH_SIZE\n",
    "            batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "        th.nn.utils.clip_grad_norm_(trainable_parameters, MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += batch_loss\n",
    "        if batch_i % LOSS_REPORT_RATE == 0:\n",
    "            time_since_start = time.time() - start_time\n",
    "            print(f\"Time: {time_since_start:.2f}, Batches: {batch_i}, Avrg loss: {loss_sum / LOSS_REPORT_RATE:.4f}\")\n",
    "            loss_sum = 0\n",
    "\n",
    "    state_dict = policy.state_dict()\n",
    "    th.save(state_dict, out_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "attention_heads': 16,\n",
    "  'attention_mask_style': 'clipped_causal',\n",
    "  'attention_memory_size': 256,\n",
    "  'diff_mlp_embedding': False,\n",
    "  'hidsize': 2048,\n",
    "  'img_shape': [128, 128, 3],\n",
    "  'impala_chans': [16, 32, 32],\n",
    "  'impala_kwargs': {'post_pool_groups': 1},\n",
    "  'impala_width': 8,\n",
    "  'init_norm_kwargs': {'batch_norm': False, 'group_norm_groups': 1},\n",
    "  'n_recurrence_layers': 4,\n",
    "  'only_img_input': True,\n",
    "  'pointwise_ratio': 4,\n",
    "  'pointwise_use_activation': False,\n",
    "  'recurrence_is_residual': True,\n",
    "  'recurrence_type': 'transformer',\n",
    "  'timesteps': 128,\n",
    "  'use_pointwise_layer': True,\n",
    "  'use_pre_lstm_ln': False},\n",
    "\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = th.nn.MultiheadAttention(\n",
    "            embed_dim=512,  # in pytorch, its split across heads\n",
    "            num_heads=16,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            add_zero_attn=False,\n",
    "            kdim=128*16,vdim=128*16,\n",
    "            batch_first=True,\n",
    "            device=DEVICE,\n",
    "            dtype=th.float32)\n",
    "\n",
    "tokensL = th.normal(0,0.02,[2,4,512])\n",
    "tokensV = th.normal(0,0.02,[2,5,2048])\n",
    "\n",
    "attn_mask = th.zeros([4,5])\n",
    "attn_mask[-1,:] = 1\n",
    "print(attn_mask)\n",
    "\n",
    "out = attention(query=tokensL, key=tokensV, value=tokensV, attn_mask=attn_mask)\n",
    "print(out)\n",
    "th.isnan(out[0]).any()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMW0NSERlVQQUPphW+YKITN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('minerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5771c508f711f7da473bf0954744d30385b19c5daa239986e839c4dfdaff2669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
